<!doctype html><html lang=en-gb><title>What I learnt writing machine learning tooling | George Thomas</title><meta charset=utf-8><meta name=generator content="Hugo 0.115.3"><meta name=viewport content="width=device-width,initial-scale=1"><link rel=stylesheet href=https://geotho.github.io/css/index.css><link rel=stylesheet href=https://geotho.github.io/css/classes.css><link rel=canonical href=https://geotho.github.io/post/2020-08-09-what-i-learnt-data-science-tooling/><link rel=alternate type=application/rss+xml href title="George Thomas"><link rel=icon type=image/svg+xml href=/favicon.svg><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.css integrity=sha384-zB1R0rpPzHqg7Kpt0Aljp8JPLqbXI3bhnPWROx27a9N0Ll6ZP/+DiW/UqRcLbRjq crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.js integrity=sha384-y23I5Q6l+B6vatafAwxRu/0oK/79VlbSz7Q9aiSZUvyWYIYsd+qj+o24G5ZU2zJz crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/auto-render.min.js integrity=sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI crossorigin=anonymous onload=renderMathInElement(document.body)></script>
<script async src="https://www.googletagmanager.com/gtag/js?id=G-DXK82H4CPF"></script>
<script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-DXK82H4CPF",{anonymize_ip:!1})}</script><body><header class=icons><a href=https://geotho.github.io/>George Thomas</a><nav><a href=/about/>About</a>
<a href=/projects/>Projects</a></nav></header><article><header><h1>What I learnt writing machine learning tooling</h1><time datetime=2020-08-09T16:33:10+01:00>August 09, 2020</time></header><h2 id=project-overview->Project overview ğŸŸ</h2><p>I spent about a year working on machine learning tooling.
The goals of the project were:</p><ol><li>Shorten time from a prototype (typically a Jupyter Notebook)
to production code.</li><li>Unify access to production data, providing security, auditability and
telemetry.</li><li>Provide a secure, scalable, multi-tenant platform for hosting models,
potentially from third-party users.</li></ol><h2 id=technology-learnings->Technology learnings ğŸ‘©ğŸ»â€ğŸ’»</h2><p>We built a system on top of Kubernetes for model storing and serving.
We wrote libraries and command line utilities which allowed data scientists to
more easily containerise their code.</p><p>Here are some things I would think twice about if I had to do the project again.</p><h3 id=1-write-command-line-utilities-in-something-other-than-python->1. Write command line utilities in something other than Python ğŸ</h3><p>We initially thought it would be useful to write our command line utilities in Python because:</p><ol><li>Our users typically use Python, so could reuse our code in their own scripts.</li><li>The source code is editable, so users could self-serve bug fixes.</li></ol><p>However, (1) was more of a curse than a blessing,
and (2) wasn&rsquo;t as useful as we first thought.</p><p>Distributing apps written in Python <a href=https://xkcd.com/1987/>remains an unsolved problem</a>.
By default, <code>pip install</code> uses a system-level dependency cache.
When you <code>pip install</code> two different projects that require different
versions of the same dependency (e.g. <code>requests</code>), <code>pip</code> uninstalls one to
install the other.</p><p>Data scientists are probably already circumventing this problem by using
VirtualEnv, Conda or poetry. But making your command line utility work with
all flavours of these â€” and working out which environment the utility should be
installed into â€” can be tricky.</p><p>ğŸ‘ <strong>Learning:</strong> use a language that you can easily cross-compile to a statically-linked binary. Go or Rust are good candidates here.</p><h3 id=2-if-using-docker-provide-good-support->2. If using Docker, provide good support ğŸš¢</h3><p>If you&rsquo;re enabling data scientists to containerise and deploy models, you&rsquo;re
likely using Docker.</p><p>It&rsquo;s useful to have a deep understanding of how Docker works, but your users may
not share your understanding.</p><p>Some recurring issues that our users encountered were:</p><ul><li>installing dependencies during <code>docker build</code> from private Git repos;</li><li>&ldquo;no space left on device&rdquo; when using Docker on OS X;</li><li>using <code>docker run -it &lt;container-id> /bin/bash</code> to debug a failing <code>docker build</code>;</li><li>inadvertantly creating massive build contexts and not using <code>.dockerignore</code>;</li><li>typical <code>Dockerfile</code> gotachas, including:<ul><li>leveraging caching by putting the commonly-rerun steps at the end;</li><li>using builder images to reduce image size;</li><li>leaving private credentials accessible in the resulting image.</li></ul></li></ul><p>ğŸ‘ <strong>Learning:</strong> Be aware of how much implicit knowledge is required to be
productive using Docker. Plan to fill this gap with docs or training.</p><h3 id=3-think-carefully-about-kubernetes-testing-strategies->3. Think carefully about Kubernetes testing strategies â›µï¸</h3><p>We wrote tools that deployed containers and services into a Kubernetes cluster
(also using Istio as a service mesh). We wanted to write integration tests that
covered all features in our offering:</p><ul><li>building and running Docker containers;</li><li>deploying the containers on a Kubernetes cluster;</li><li>managed access to external data resources;</li><li>model execution.</li></ul><p>On each CI run, we span-up a Kubernetes cluster (initially Minikube, then
MicroK8s, then KinD ğŸ¤¦ğŸ»â€â™‚ï¸) and installed Istio. Our CI execution time quickly
ballooned to ~30 minutes.</p><p>An alternative would be to test continuously in a non-production environment.</p><p>I&rsquo;d like to investigate an approach where CI runs separately for cluster-setup
scripts and application code, and CI by default runs against a preexisting
cluster.</p><p>ğŸ‘ <strong>Learning:</strong> It is difficult to naÃ¯vely test applications that interact with
Kubernetes in a CI-acceptable timeframe. It&rsquo;s worth investing in innovative
approaches here.</p><h2 id=people-learnings>People learnings</h2><p>I also learnt more about the non-coding parts of building technology.</p><h3 id=1-build-a-product-your-users-love->1. Build a product your users love ğŸ’Œ</h3><p>I agree with Paul Graham: <a href=http://www.paulgraham.com/good.html>make something people want</a>. This is as true within larger companies as it is in startups.</p><p>Treat your software as a product. Treat your users as customers. Even if they
are internal.</p><p>Start by making a small pool of users very happy and more productive.
This leads to traction and buy-in across an organisation.</p><p>ğŸ‘ <strong>Learning:</strong> Build software your users love to maximise traction.</p><h3 id=2-get-feedback-early->2. Get feedback early â°</h3><p>Getting feedback early is the best way to ensure you&rsquo;re building something
people want.</p><p>We got some elements of the user experience wrong. Focus on being wrong quickly.
Users are good are telling you want they don&rsquo;t want, or what doesn&rsquo;t work for
them. The quicker you finish building the wrong thing, the earlier you can start
building the right thing.</p><p>Some ways of acquiring feedback, in approximate order of efficacy:</p><ol><li>Pair programming / watching users use the software</li><li>Answering users&rsquo; questions / debugging their painpoints</li><li>Ask users to document friction as they use the software</li></ol><p>ğŸ‘ <strong>Learning:</strong> There&rsquo;s a high likelihood that parts of your user experience
are suboptimal. Figure out which by prototyping functionality and observing users.</p><h3 id=3-encourage-public-question-asking->3. Encourage public question asking ğŸ“£</h3><p>Many users ask questions or raise issues privately i.e. through email or Slack
direct message etc. They might do this because:</p><ol><li>You specifically have helped them with an issue in the past.</li><li>They&rsquo;re afraid of asking a &ldquo;dumb question&rdquo; publicly.</li></ol><p>This behaviour should be discouraged. Instead, create a public Slack channel,
mailing list or JIRA board where users can raise issues.</p><p>This has many advantages:</p><ol><li>It reduces the <a href=https://en.wikipedia.org/wiki/Bus_factor>bus factor</a>.</li><li>Issues / friction / painpoints in the tooling become visible to the whole team.</li><li>Solutions to reoccuring questions can be reused (via FAQs).</li><li>Users can help each other with questions.</li></ol><p>ğŸ‘ <strong>Learning:</strong> Encourage users to ask questions publicly. The benefits will
save time and help build a better product.</p><h2 id=here-be-dragons->Here be dragons ğŸ²</h2><p>Our work only dealt with synchronous streaming use cases. There&rsquo;s a whole
universe of machine learning tooling that was out of scope for us.</p><p>I&rsquo;ve highlighted a few interesting areas of exploration.</p><h3 id=data-lineage->Data lineage ğŸ“š</h3><p>How do you keep track of what data went into building a model?
If you sample data, how can you build confidence in your sampling algorithms
and reproducibility?
Could you retrain the exact same model from scratch if necessary?</p><p><a href=https://www.pachyderm.com/>Pachyderm</a> looks like they are doing interesting
things in this area.</p><h3 id=distributed-model-training->Distributed model training ğŸ‹ğŸ»â€â™‚ï¸</h3><p>Spark clusters tend to be a constant source of reoccuring pain here.</p><p>Should data scientists be responsible for spinning up (and turning down!) a new
cluster for every training job?</p><p>How would this work in a continuous training (CT) environment?</p><p>Should those same data scientists own the infra necessary for creating Spark
clusters? It&rsquo;s not part of their core job description. But if another team
(e.g. SRE) owns it but doesn&rsquo;t use it, how will they be incentivised to fix
the problems?</p><p>If you want to reuse a persistent cluster, how do you handle scheduling and
dependency management?</p><p>It&rsquo;s worth considering Apache Beam and Kubeflow Pipelines as alternatives.
But the amount of infra necessary seems <a href=https://cloud.google.com/solutions/machine-learning/architecture-for-mlops-using-tfx-kubeflow-pipelines-and-cloud-build>gargantuan</a>.</p></article></body></html>